# -*- coding: utf-8 -*-
"""benshfinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PA4BuDbq9QykHTvV0ivqvEyvlRS4B5qY

# Airline Line Ticket Price Prediction Model Using Kaggle Dataset

Importing necessary library and dataset
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime

"""Importing the required dataset

The dataset that we are working on is a static dataset of airline companies
"""

data_train=pd.read_excel('/content/Data_Train.xlsx')

#pip install -U dataprep

#from dataprep.eda import create_report
#import pandas as pd
#dataframe = pd.read_excel('/content/Data_Train.xlsx')
#create_report(dataframe)

data_train.shape

data_train.loc[100]

#from dataprep.eda import create_report
#import pandas as pd
#dataframe = pd.read_excel("/content/Data_Train.xlsx")
#create_report(dataframe)



"""#Data preparation and exploration,

Clean, Prepare & Manipulate Data

Real-world data often has unorganized, missing, or noisy elements. Therefore, for Machine Learning success, after we chose our data, we need to clean, prepare, and manipulate the data.

This process is a critical step, and people typically spend up to 80% of their time in this stage. Having a clean data set helps with your model’s accuracy down the road.
"""

data_train.info()

"""As we can see the dataset has different data type"""

data_train.describe()

data_train.isnull().sum()

"""Missing values can be handled by deleting the rows or columns having null values. If columns have more than half of rows as null then the entire column can be dropped. The rows which are having one or more columns values as null can also be dropped.

As less missing values,I can directly drop these table
"""

data_train.dropna(inplace=True)

data_train.isnull().sum()

"""We can see from the dataframe that the date_of_journey feature is object data type.We need to convert into time-stamp format because we need to  use this column properly for prediction,and also our machine learning 
 model will not be able to understand these string values,it can only understand Time-stamp.

For this we require pandas to_datetime to convert object data type to datetime dtype.
"""

def deparrtime(x):
    x=x.strip()
    tt=(int)(x.split(':')[0])
    if(tt>=16 and tt<21):
        x='Evening'
    elif(tt>=21 or tt<5):
        x='Night'
    elif(tt>=5 and tt<11):
        x='Morning'
    elif(tt>=11 and tt<16):
        x='Afternoon'
    return x

data_train['Dep_']=data_train['Dep_Time'].apply(deparrtime)
data_train['Arrival_']=data_train['Arrival_Time'].apply(deparrtime)

def to_datetime_type(col):
  data_train[col]=pd.to_datetime(data_train[col])

data_train.columns

for i in ['Date_of_Journey','Dep_Time','Arrival_Time']:
   to_datetime_type(i)

data_train.dtypes

data_train['day']=data_train['Date_of_Journey'].dt.day

data_train['month']=data_train['Date_of_Journey'].dt.month

data_train['weekday']= pd.to_datetime(data_train.Date_of_Journey, format='%d/%m/%Y').dt.weekday

data_train['year']=data_train['Date_of_Journey'].dt.year

data_train

data_train.dtypes

data_train['year'].unique()

data_train['month'].unique()

data_train['Destination'].replace('Delhi','New Delhi',inplace=True)

data_train['Source'].replace('Delhi','New Delhi',inplace=True)

data_train['Additional_Info'].unique()

data_train['Additional_Info'].replace({"No info":'No Info'}, inplace = True)

data_train['month'].value_counts()

"""Since we have converted Date_of_Journey column into integers, we can drop the column as it is of no use."""

data_train.drop('Date_of_Journey',axis=1, inplace=True)

"""Droping the year column as it is same all over the data."""

data_train.drop('year',axis=1, inplace=True)

data_train.head()

"""Extracting values from Dep_Time and Arrival_Time as it is also similar to date of journey

We will create a function to seperate the features from the date fields.
"""

def hours_extract(df,column):
  df[column+"_hour"]=df[column].dt.hour

def minutes_extract(df,column):
  df[column+"_minute"]=df[column].dt.minute



hours_extract(data_train,'Dep_Time')
hours_extract(data_train,'Arrival_Time')

minutes_extract(data_train,'Dep_Time')
minutes_extract(data_train,'Arrival_Time')

data_train.head()

#data_train.iloc[2029]

data_train.describe()

"""Now that we have filtered the necessary components of the label we can now drop the unnessary labels"""

data_train.drop('Dep_Time',axis=1,inplace=True)
data_train.drop('Arrival_Time',axis=1,inplace=True)

data_train.head()



"""We again need to apply pre-processing in the duration column to seperate the duration hours and minutes"""

duration=list(data_train['Duration'])

for i in range(len(duration)):
  if len((duration[i]).split(' '))==2:
    pass
  else:
    if 'h' or 'H' in duration[i]:
      duration[i]=duration[i] + ' 0m'
    else:
      duration[i]='0h ' + duration[i]

data_train['Duration']=duration

data_train['Duration']

def hour(x):
    return x.split(' ')[0][0:-1]

def minute(y):
    return y.split(' ')[1][0:-1]

data_train['travel_hours']=data_train['Duration'].apply(hour)

data_train['travel_minutes']=data_train['Duration'].apply(minute)

data_train.head()

data_train.dtypes

data_train['Source'].unique()

data_train['Destination'].unique()

"""Need to convert the travel hours and minutes column to integer data type"""

data_train['travel_hours']=data_train['travel_hours'].astype(int)
data_train['travel_minutes']=data_train['travel_minutes'].astype(int)

data_train.dtypes

"""Handling categorical data

We are using 2 main Encoding Techniques to convert Categorical data into some numerical format.

1.   Label Encoding for Ordinal Data
2.   OneHotEncoding for Nominal Data
"""

categorical_columns=[Columns for Columns in data_train.columns if data_train[Columns].dtype=='O']

categorical_columns

categorical_data=data_train[categorical_columns]

categorical_data.head()

categorical_data['Airline'].value_counts()

plt.figure(figsize = (15, 10))
plt.title('Count of flights month wise')
ax=sns.countplot(x = 'month', data = data_train)
plt.xlabel('Month')
plt.ylabel('Count of flights')
for p in ax.patches:
    ax.annotate(int(p.get_height()), (p.get_x()+0.25, p.get_height()+1), va='bottom',
                    color= 'black')

monthly_avg=data_train.groupby(['month']).agg({'Price':np.mean}).reset_index()

"""Analysis :
We see that the total count of flight is maximum towards the month-May which can also be concluded from the above bar plot which shows that the sum of fare is maximum in May.
This can be due to : Summer vacations in the month of may for schools/colleges, hence most families are also generally going for vacations around this time.
The count of flights is lowest on the month of April, this can be because : Schools,colleges have their final exams around this time, offices are mostly busy in the month of April as it is the end of Quarter 1.
"""

monthly_avg.plot(x='month',y='Price',figsize=(6,6))

"""We see that the average fare price is highest the month of March , this can be because people usually book the fights 2-3 months prior to their date of journey which leads to higher demand and hence higher fare prices.(since the count of tickets booked for travelling in May is the most, prior bookings are done in March)

#Destination vs Price
"""

sns.catplot(y='Price',x='Destination',data= data_train.sort_values('Price',ascending=False),kind="boxen",height=6, aspect=3)
plt.show

"""The airfare price range in Delhi & New Delhi is the maximum

#Source vs Price
"""

sns.catplot(y='Price',x='Source',data= data_train.sort_values('Price',ascending=False),kind="boxen",height=6, aspect=3)
plt.show

"""#Airline vs Price Analysis

"""

#Count of flights v/s Airline
plt.figure(figsize = (10, 10))
plt.title('Count of flights with different Airlines')
ax=sns.countplot(x = 'Airline', data =data_train)
plt.xlabel('Airline')
plt.ylabel('Count of flights')
plt.xticks(rotation = 90)
for p in ax.patches:
    ax.annotate(int(p.get_height()), (p.get_x()+0.25, p.get_height()+1), va='bottom',
                    color= 'black')

"""Jet Airways have the highest no. of flights followed by Indigo and Air India"""

#Airline vs Price
plt.figure(figsize=(20,10))
sns.boxplot(y='Price',x='Airline',data=data_train.sort_values('Price',ascending=False))

"""Jet airways and Air India are full service airlines are and always highly priced due to various amenities they provide. Low-cost carriers like indigo and spicejet have a lower and similar fare range

Conclusion--> From graph we can see that Jet Airways Business have the highest Price., Apart from the first Airline almost all are having similar median
"""

pip install chart-studio

from chart_studio.plotly import iplot

import plotly.express as px

"""Perform Total_Stops vs Price Analysis"""

dep=sns.barplot(x='Dep_', y='Price', data=data_train)
dep.set_ylabel('Price')
dep.set_xlabel('Time of deptarture')
dep.set_xticklabels(dep.get_xticklabels(), rotation=80)

plt.figure(figsize=(20,10))
sns.boxplot(y='Price',x='Total_Stops',data=data_train.sort_values('Price',ascending=False))

categorical_data['Airline'].unique()

"""get_dummies"""

airline=pd.get_dummies(categorical_data['Airline'],drop_first=True)
airline.head()

# time of departure v/s count of flights
top_time=data_train.Dep_.value_counts().head(10)
top_time

#TIME OF ARRIVAL V/S average price
Ta=sns.barplot(x='Arrival_', y='Price', data=data_train)
Ta.set_title('TIME OF ARRIVALV/S PRICE')
Ta.set_ylabel('Price')
Ta.set_xlabel('Arrival_time')
Ta.set_xticklabels(Ta.get_xticklabels(), rotation=80)

"""Early Morning flights are always cheaper and so are midnight flight prices.¶
Evening flight fares are expensive due to more demand and is the most convenient time to tarvel for most people.
"""



categorical_data['Source'].value_counts()

"""#Source vs Price"""

plt.figure(figsize=(15,8))
plt.subplot(1,2,1)
data_train['Source'].value_counts().plot.pie(autopct='%1.1f%%')
centre=plt.Circle((0,0),0.7,fc='white')
fig=plt.gcf()
fig.gca().add_artist(centre)
plt.subplot(1,2,2)
sns.countplot(x='Source',data=data_train)
data_train['Source'].value_counts()

plt.figure(figsize=(20,10))
sns.boxplot(x='Source',y='Price',data=data_train.sort_values('Price',ascending=False))

TS=sns.barplot(x='Total_Stops', y='Price', data=data_train)
TS.set_title('NO. OF STOPS V/S PRICE')
TS.set_ylabel('Price')
TS.set_xlabel('Total_Stops')
TS.set_xticklabels(TS.get_xticklabels(), rotation=80)

plt.figure(figsize=(15,8))
plt.subplot(1,2,1)
data_train['Total_Stops'].value_counts().plot.pie(autopct='%1.1f%%')
centre=plt.Circle((0,0),0.7,fc='white')
fig=plt.gcf()
fig.gca().add_artist(centre)
plt.subplot(1,2,2)
sns.countplot(x='Total_Stops',data=data_train)
data_train['Total_Stops'].value_counts()

"""As a direct/non-stop flight is accounting for fare of only one flight for a trip, its average fair is the least. As the no. of stops/layovers increase, the fare price goes up accounting for no. of flights and due to other resources being used up for the same."""

data_train["day"].unique()

#Count of flights with different dates
plt.figure(figsize = (15, 10))
plt.title('Count of flights with different dates')
ax=sns.countplot(x = 'day', data =data_train)
plt.xlabel('Journey_day')
plt.ylabel('Count of flights')
plt.xticks(rotation = 90)
for p in ax.patches:
    ax.annotate(int(p.get_height()), (p.get_x()+0.25, p.get_height()+1), va='bottom',
                    color= 'black')

#Journey_Day v/s Average price
Ja=sns.barplot(x='day', y='Price', data=data_train)
Ja.set_title('Price of flights with different datess')
Ja.set_ylabel('Price')
Ja.set_xlabel('date')
Ja.set_xticklabels(Ja.get_xticklabels(), rotation=80)

"""It looks like that there's a trend in the air fare when compared to the day of respective months, prices are higher in the start of month but this is not a trend if you see from the broader perspective as this might be due to various reasons. For eg. the date of Journey is 10th March and people are booking towards 5th March or so, this will lead to higher flight prices.(Prices increase as near you date of booking is to the date of journey). So flight prices don't follow any particular pattern towards any time of the month."""

#price vs additional info
plt.figure(figsize=(15,6))
sns.boxenplot(x='Additional_Info',y='Price',data=data_train)
plt.xticks(rotation=45)

"""Flight price is higheer where the additional info contains Bussiness class followed by 1 long layover or 1 short/long layover."""

plt.figure(figsize=(15,6))
sns.countplot(data_train['Source'],hue='Total_Stops',data=data_train)
plt.legend(loc='upper right')

"""Banglore is the only source of flights where the most of the flights have no stops. Almost all the flights from chennai have no stops."""



categorical_data.head()

source=pd.get_dummies(categorical_data['Source'],drop_first=True)
source=source.rename(columns={"Kolkata": "Kolkata_s","New Delhi":"New Delhi_s"})
source.head()

plt.figure(figsize=(15,8))
plt.subplot(1,2,1)
data_train['Destination'].value_counts().plot.pie(autopct='%1.1f%%')
centre=plt.Circle((0,0),0.7,fc='white')
fig=plt.gcf()
fig.gca().add_artist(centre)
plt.subplot(1,2,2)
sns.countplot(x='Destination',data=data_train)
data_train['Destination'].value_counts()

plt.figure(figsize=(8,6))
sns.histplot(data_train['Price'],kde=True,color='k')
print('Minimum',data_train['Price'].min())
print('Maximum',data_train['Price'].max())

#plt.figure(figsize=(8,6))
#sns.histplot(['Duration_mins'],kde=True,color='k')
#print('Minimum',df['Duration_mins'].min())
#print('Maximum',df['Duration_mins'].max())

categorical_data['Destination'].value_counts()

"""Final destination of majority of flights is Cochin. There are two values for Delhi destination which needs to be corrected,

Destination is also a nominal data
"""

destination=pd.get_dummies(categorical_data['Destination'],drop_first=True)
destination.head()

"""drop_first=True is important to use, as it helps in reducing the extra column created during dummy variable creation."""

#plt.figure(figsize=(15,8))
#sns.countplot(data_train['Route'],color='k')
#data_train['Route'].value_counts().head()

"""DEL → BOM → COK and BLR → DEL seems to be the busiest route, while there are only 6 routes which is taken by more than 500 flights"""

#categorical_data['Route'].unique().tolist()

"""We now know that there are 128 unique routes"""

categorical_data['Route1']=categorical_data['Route'].str.split('→').str[0]

categorical_data['Route1']



categorical_data

import warnings 
from warnings import filterwarnings
filterwarnings('ignore')

categorical_data['Route2']=categorical_data['Route'].str.split('→').str[1]
categorical_data['Route3']=categorical_data['Route'].str.split('→').str[2]
categorical_data['Route4']=categorical_data['Route'].str.split('→').str[3]
categorical_data['Route5']=categorical_data['Route'].str.split('→').str[4]

categorical_data['Route2'].unique()

categorical_data

categorical_data['Route1'].fillna('None',inplace=True)
categorical_data['Route2'].fillna('None',inplace=True)
categorical_data['Route3'].fillna('None',inplace=True)
categorical_data['Route4'].fillna('None',inplace=True)
categorical_data['Route5'].fillna('None',inplace=True)

"""Now lets find out how many values are there in each category"""

for values in categorical_data.columns:
  print(values,len(categorical_data[values].value_counts()))

"""As there are lot of values in the route feature we have to apply label encoding."""

from sklearn.preprocessing import LabelEncoder
en=LabelEncoder()

for i in ['Dep_','Arrival_']:
  categorical_data[i]=en.fit_transform(categorical_data[i])

for i in ['Route1','Route2','Route3','Route4','Route5']:
  categorical_data[i]=en.fit_transform(categorical_data[i])

categorical_data.dtypes

categorical_data.drop('Route',axis=1,inplace=True)

plt.figure(figsize=(15,8))
sns.countplot(x='Additional_Info',data=data_train)
plt.xticks(rotation=45)
data_train['Additional_Info'].value_counts()

"""Majority of the flights do not provide additional info, whereas some proved information such as In-flight meal not included, Bussiness class which colud be helpful in determining the price of the flight."""



#data_train['Additional_Info'].value_counts()

#Add_info = categorical_data['Additional_Info']
#Add_info = pd.get_dummies(Add_info, drop_first = True)

#Add_info

categorical_data['Total_Stops'].value_counts()

"""This seems to be ordinal category hence we can perform label encoding"""

dict={'non-stop':0,'2 stops':2, '1 stop':1, '3 stops':3, '4 stops':4}

categorical_data['Total_Stops']=categorical_data['Total_Stops'].map(dict)

continous_columns=[Columns for Columns in data_train.columns if data_train[Columns].dtype!='O']

continous_columns

data_train[continous_columns].head()

""" #Concatenate dataframe --> categorical + Airline + Source + Destination#"""

FE_data_train=pd.concat([categorical_data,airline,source,destination,data_train[continous_columns]],axis=1)

FE_data_train.head()

def duration(test):
    test = test.strip()
    total=test.split(' ')
    to=total[0]
    hrs=(int)(to[:-1])*60
    if((len(total))==2):
        mint=(int)(total[1][:-1])
        hrs=hrs+mint
    test=str(hrs)
    return test
FE_data_train['Duration']=FE_data_train['Duration'].apply(duration)

FE_data_train['Duration']=pd.to_numeric(FE_data_train['Duration'])

FE_data_train['Price']=pd.to_numeric(FE_data_train['Price'])

import scipy.stats as stats
graph = sns.jointplot(data=FE_data_train,x='Duration', y='Price')
r, p = stats.pearsonr(x=FE_data_train['Duration'],y=FE_data_train['Price'])
# if you choose to write your own legend, then you should adjust the properties then
phantom, = graph.ax_joint.plot([], [], linestyle="", alpha=0)
# here graph is not a ax but a joint grid, so we access the axis through ax_joint method

graph.ax_joint.legend([phantom],['r={:f}, p={:f}'.format(r,p)])

"""We know that duration( or distance) plays a major role in affecting air ticket prices but we see no such pattern here, as there must be there are other significant factors affecting air fare like type of airline, destination of flight, date of journey of flight(higher if collides with a public holiday)"""

FE_data_train.dtypes

#import seaborn as sns
#import scipy.stats as stats

#sns.set(style="darkgrid", color_codes=True)
#j = sns.jointplot('Duration', 'Price', data = FE_data_train, kind='reg', height=8)
#j.annotate(stats.pearsonr)
#plt.show()

"""We get p-value < 0.05, hence we accept H1 and say the target variable and continuous independent variable are correlated. r = 0.51 says they are moderately related."""



#FE_data_train.drop('Duration',axis=1,inplace=True)
FE_data_train.head()

FE_data_train.shape

FE_data_train.columns.value_counts()

FE_data_train.drop('Destination',axis=1,inplace=True)
FE_data_train.drop('Source',axis=1,inplace=True)
FE_data_train.drop('Airline',axis=1,inplace=True)
FE_data_train.drop('Additional_Info',axis=1,inplace=True)

"""#Outlier Detection"""

#price outlier check
Q1=FE_data_train['Price'].quantile(0.25)
Q3=FE_data_train['Price'].quantile(0.75)
IQR=Q3-Q1

print(Q1)
print(Q3)
print(IQR)

def distribution_plot(df,column):
  fig,(axis1,axis2)=plt.subplots(2,1)
  sns.distplot(df[column],ax=axis1)
  sns.boxplot(df[column],ax=axis2)

plt.figure(figsize=(50,50))
distribution_plot(FE_data_train,'Price')

"""Dealing with outliers

As we can see there are so many outliers with respect to the price
"""

FE_data_train['Price']=np.where(FE_data_train['Price']>=40000,FE_data_train['Price'].median(),FE_data_train['Price'])

plt.figure(figsize=(50,50))
distribution_plot(FE_data_train,'Price')

"""Now lets seperate our independent and dependent data"""

X=FE_data_train.drop('Price',axis=1)
X





X.shape

X.isnull().sum()

y=FE_data_train['Price']
y

y.isnull().sum()

y.describe()

"""#Feature Selection

Now we need to find the best features that will contribute to find a perfect relation with target variable

We do this also to reduce the curese of dimentionality to get rid of duplicate features

Correlation test:
Chi-square test for categorical variables :
Hypothesis testing :

HO: The two variables are independent

H1: The two variables are dependent

If p-value <0.01(we take confidence interval as 99%,and alpha =0.01, as from our analysis see that keeping significance level as 0.05,many variables became depedent)

so then we reject the null hypothesis saying that 2 variables are dependent.

There should be no dependencies between Independent variables.

So, we check variables who are highly dependent with other variables, we remove them
"""

from sklearn.feature_selection import chi2
n= 10
for i in range(0,9):
    chi_x=FE_data_train.iloc[:,i+1:n]
    chi_y=FE_data_train.iloc[:,i]
    chi_scores = chi2(chi_x,chi_y)
    p_values = pd.Series(chi_scores[1],index = chi_x.columns)
    print("for",i)
    print(p_values)
    for j in range (0, len(p_values)):
        if (p_values[j]<0.01):
            print(p_values[j])

"""Checking the p-values, we drop: "Airline","Source","Destination","Total_Stops","Journey_Month","Journey_Day","Arrival_Time"

Comparison test :
Anova test
It is carried out to compare between each groups in a categorical variable.

ANOVA only lets us know the means for different groups are same or not. It doesn’t help us identify which mean is different.

Hypothesis testing :

H0: means of all levels of the categorical variable is same

H1: mean of at least one level is different

If p-value < 0.05 then we reject the null hypothesis.
"""

import statsmodels.api as sm
from statsmodels.formula.api import ols
model = ols('Price ~ C(Dep_)+C(weekday)',data=FE_data_train).fit()
aov_table = sm.stats.anova_lm(model)
aov_table

probanova=list(aov_table["PR(>F)"])

for i in range(0,3):
    if probanova[i]>0.05:
        print(i)
    else:
        print('No values greater than 0.05')

"""As p-value<0.05 for all the variables, we reject H0 and hence no variables are removed

Correlation test:
Multicollinearity Test
It occurs when two or more independent variables are highly correlated with one another in a regression model.

if VIF is 1 --- Not correlated to any of the variables.

if VIF is between 1-5 --- Moderately correlated.

if VIF is above 5 --- Highly correlated.
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
def calc_vif(X):
    vif = pd.DataFrame()
    vif["variables"] = FE_data_train.columns
    vif["VIF"] = [variance_inflation_factor(FE_data_train.values, i) for i in range(FE_data_train.shape[1])]
    return(vif)

df1=FE_data_train.drop(["Price"],axis=1)
calc_vif(df1)

from sklearn.feature_selection import mutual_info_classif

"""####  Feature Selection using Information Gain,"""

FSC=mutual_info_classif(X,y)

FS=pd.DataFrame(FSC,index=X.columns)

FS

FS.columns=['Significance']
FS.sort_values(by='Significance',ascending=False,inplace=True)

fig ,f=plt.subplots(figsize=(10,15))
sns.heatmap(FS,annot=True,fmt="",cmap='RdYlGn',linewidths=0.60,ax=f)
plt.show()

"""##Heatmap"""

# Finding correlation betwwen Independent and Dependent Feature

plt.figure(figsize=(18,18))
sns.heatmap(data_train.corr(),annot=True,cmap='RdYlGn')

plt.show()

plt.figure(figsize=(18,18))
plt.title('Covariance Matrix\n')
sns.heatmap(data_train.cov(),annot=True)
plt.show()

"""#Model Train,Test split"""

from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=34)

X_train.shape

X_test.shape

X_test.isnull().sum()

X_train.head()

from sklearn import preprocessing

#from sklearn.model_selection import train_test_split
#x1 = FE_data_train.drop('Price',axis=1).values
#min_max_scaler = preprocessing.MinMaxScaler()
#x_scaled = min_max_scaler.fit_transform(x1)
#X1 = pd.DataFrame(x_scaled)
#y1 = FE_data_train['Price'].values

#model = sm.OLS(y1,X1).fit()
#model.summary()

from sklearn.ensemble import ExtraTreesRegressor
selection = ExtraTreesRegressor()
selection.fit(X, y)

print(selection.feature_importances_)

plt.figure(figsize = (12,8))
feat_importances = pd.Series(selection.feature_importances_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()

import pickle

import xgboost
from xgboost import XGBRegressor

def prediction(best_model,dump):
  model=best_model.fit(X_train,y_train)
  print('Training score is : {}'.format(model.score(X_train,y_train)))
  print('Test score is : {}'.format(model.score(X_test,y_test)))
  print('\n')
  y_predict=model.predict(X_test)
  print('The predicted values are : {}'.format(y_predict))
  print('\n')
  r2_score=metrics.r2_score(y_test,y_predict)
  print('coefficient of determination : {}'.format(r2_score))
  MAE=metrics.mean_absolute_error(y_test,y_predict)
  print('Mean absolute error : {}'.format(MAE))
  MSE=metrics.mean_squared_error(y_test,y_predict)
  print('Mean squared error : {}'.format(MSE))
  RMSE=np.sqrt(metrics.mean_squared_error(y_test,y_predict))
  print('root mean square error : {}'.format(RMSE))


  print('\n')
  number_of_observations=100
  x_ax = range(len(y_test[:number_of_observations]))
  plt.plot(x_ax, y_test[:number_of_observations], label="original")
  plt.plot(x_ax, y_predict[:number_of_observations], label="predicted")
  plt.title("Flight Price test and predicted data")
  plt.xlabel('Observation Number')
  plt.ylabel('Price')
  plt.legend()
  plt.show()

  if dump==1:
    file=open('C:/Users/benes/Desktop/final year project/model.pkl','wb')
    pickle.dump(model,file)

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
from sklearn.svm import SVR

prediction(XGBRegressor(),0)

prediction(RandomForestRegressor(),0)

prediction(LinearRegression(),0)

#prediction(SVR(),0)

#prediction(LogisticRegression(),0)

"""#XGB hyperparameter tuning and cross validation"""

x_xb=np.array(X_train)
  y_xb=np.array(y_train)

from sklearn.model_selection import GridSearchCV

xgb = XGBRegressor()

best_xgb = GridSearchCV(xgb, param_grid={'learning_rate':[0.01,0.05,0.1],'max_depth':[1,2,3],'n_estimators':[100,200,500]}, cv=5, n_jobs=-1)

best_xgb.fit(x_xb,y_xb)

best_xgb.best_params_

from sklearn.model_selection import RandomizedSearchCV , cross_val_score

scores = cross_val_score(best_xgb.best_estimator_, x_xb, y_xb, cv=5)
print("XGBoost Cross validation score: {0:.2%} (+/- {1:.2%})".format(np.mean(scores), np.std(scores)*2))

scores = cross_val_score(best_xgb.best_estimator_, x_xb, y_xb, cv=5)
scores

y_predict1=best_xgb.predict(x_xb)
y_predict1

metrics.r2_score(y_xb,y_predict1)

#prediction(Lasso(),0)

prediction(DecisionTreeRegressor(),0)

prediction(KNeighborsRegressor(),0)

"""1.Choose following method for hyperparameter tuning
    a.RandomizedSearchCV --> Fast way to Hypertune model
    b.GridSearchCV--> Slow way to hypertune my model

#Hyper parameter tuning Random forest and cross validation
"""

from sklearn.model_selection import RandomizedSearchCV , cross_val_score
from sklearn.ensemble import RandomForestRegressor

reg_rf=RandomForestRegressor()

# Number of trees in random forest
n_estimators=[int(x) for x in np.linspace(start=100,stop=1200,num=6)]

# Number of features to consider at every split
max_features=['auto','sqrt']

# Maximum number of levels in tree
max_depth=[int(x) for x in np.linspace(5,30,num=4)]

# Minimum number of samples required to split a node
min_samples_split=[5,10,15,100]

random_grid={
    'n_estimators':n_estimators,
    'max_features':max_features,
    'max_depth':max_depth,
    'min_samples_split':min_samples_split
}

# Random search of parameters, using 3 fold cross validation

RF_Airticket_price_prediction=RandomizedSearchCV(estimator=reg_rf,param_distributions=random_grid,cv=3,verbose=2,n_jobs=-1)

RF_Airticket_price_prediction.fit(X_train,y_train)

RF_Airticket_price_prediction.best_params_

prediction=RF_Airticket_price_prediction.predict(X_test)

sns.distplot(y_test-prediction)

print('Test score is : {}'.format(RF_Airticket_price_prediction.score(X_test,y_test)))

number_of_observations=50
  x_ax = range(len(y_test[:number_of_observations]))
  plt.plot(x_ax, y_test[:number_of_observations], label="original")
  plt.plot(x_ax, prediction[:number_of_observations], label="predicted")
  plt.title("Flight Price test and predicted data")
  plt.xlabel('Observation Number')
  plt.ylabel('Price')
  plt.legend()
  plt.show()

scores = cross_val_score(RF_Airticket_price_prediction.best_estimator_, X_train, y_train, cv=5)
print("rf Cross validation score: {0:.2%} (+/- {1:.2%})".format(np.mean(scores), np.std(scores)*2))

print('MAE',metrics.mean_absolute_error(y_test,prediction))
print('MSE',metrics.mean_squared_error(y_test,prediction))
print('RMSE',np.sqrt(metrics.mean_squared_error(y_test,prediction)))

plt.scatter(y_test, prediction, alpha = 0.5)
plt.xlabel("y_test")
plt.ylabel("y_pred")
plt.show()

y_test

prediction

#plt.scatter(x=y_test,y=prediction,color='r')
#plt.plot(y_test,y_test,color='b')
#plt.xlabel('Actual Price')
#plt.ylabel('Predicted Price')
#plt.title('RF')

FE_data_train.head(3)

rf=RandomForestRegressor(n_estimators=200,random_state=0,min_samples_split=10)
rf.fit(X_train,y_train)

#def pred_ints(model, X, percentile=95):
    #err_down = []
    #err_up = []
    #for x in range(len(X)):
     #   preds = []
     #   for pred in model.estimators_:
     #       preds.append(pred.predict(X.loc[x]))
      #      preds = preds.reshape(1,-1)
      #  err_down.append(np.percentile(preds, (100 - percentile) / 2. ))
      #  err_up.append(np.percentile(preds, 100 - (100 - percentile) / 2.))
    #return err_down, err_up

#err_down, err_up = pred_ints(, X_test, percentile=90)
 
#truth = y_test
#correct = 0.
#for i, val in enumerate(truth):
 #   correct += 1
#print (correct/len(truth))

X_test.head()

pred_q = pd.DataFrame()

#rf.estimators_

#err_down = []
#err_up = []
#def benesh(X,percentile=95):
  #for pred in rf.estimators_:
    #pred_q = pd.DataFrame()
    #temp = pd.Series(pred.predict(X).round(2))
    #pred_q = pd.concat([pred_q,temp],axis=1)
  #print(pred_q)#
  #err_down.append(np.percentile(pred_q, (100 - percentile) / 2. ))
  #err_up.append(np.percentile(pred_q, 100 - (100 - percentile) / 2.))
  #return err_down, err_up

for pred in rf.estimators_:
  temp = pd.Series(pred.predict(X_test).round(2))
  pred_q = pd.concat([pred_q,temp],axis=1)
pred_q.head()

quantiles= [0.025,0.05,0.95,0.975]

RF_actual_pred = pd.DataFrame()

#for q in quantiles:
 # s= pred_q.quantile(q=q,axis=1)
  #RF_actual_pred=pd.concat([RF_actual_pred,s],axis=1,sort=False)



#RF_actual_pred.head()

RF_actual_pred=pd.DataFrame()

for q in quantiles:
  s= pred_q.quantile(q=q,axis=1)
  RF_actual_pred=pd.concat([RF_actual_pred,s],axis=1)
RF_actual_pred = RF_actual_pred.set_index(y_test.index)
RF_actual_pred.columns=quantiles

RF_actual_pred.head()

RF_actual_pred['actual'] = y_test
RF_actual_pred

#RF_actual_pred['interval'] = RF_actual_pred[np.max(quantiles)]-RF_actual_pred[np.min(quantiles)]
#RF_actual_pred

RF_actual_pred['interval'] = RF_actual_pred[0.975]-RF_actual_pred[0.025]
RF_actual_pred.sort_index(ascending=True)

RF_actual_pred = RF_actual_pred.sort_values('interval')
RF_actual_pred.sort_index(ascending=True)

def showIntervals(df):
  df=df.reset_index(drop='index')
  plt.figure(figsize=(100,25))
  plt.plot(df['actual'],'go',markersize=10,label='Actual')
  plt.fill_between(np.arange(df.shape[0]),df[0.025],df[0.975],alpha=0.5,color="tan",label="Predicted interval")
  plt.xlabel("Ordered samples.")
  plt.ylabel("Values and prection intervals")
  plt.xlim([1400,1600])
  plt.ylim([1000,30000])


  plt.legend()
  plt.show()

RF_actual_pred = RF_actual_pred.round(2)
RF_actual_pred.sort_index(ascending=True)

RF_actual_pred.describe()

#RF_actual_pred = pd.DataFrame()

#RF_actual_pred.shape

#RF_actual_pred.shape

#obs = RF_actual_pred.shape[0]
#obs

#RF_actual_pred.iloc[520]['actual]

def correctpred(df):
  correct = 0
  obs = df.shape[0]
  
  for i in range(obs):
    if df.iloc[i][0.025] <= df.iloc[i]['actual'] <= df.iloc[i][0.975]:
        correct += 1
  print(correct/obs)

correctpred(RF_actual_pred)

"""#Prediction intervals for random samples"""

#err_down, err_up = benesh(X_test,percentile=50)
#truth = y_test
#print(err_down)
#correct = 0.
#for i, val in enumerate(truth):
    #if err_down[i] <= val <= err_up[i]:
       # correct += 1
#print(correct/len(truth))

import pickle

"""#Model Deploy"""

# open a file, where you want to store the data
file=open('RF_Airticket_price_prediction.pkl','wb')

import gzip, pickle
with gzip.open('RF_Airticket_price_prediction.pkl', 'wb') as rfp:
    pickle.dump(RF_Airticket_price_prediction,rfp)

with gzip.open('RF_Airticket_price_prediction.pkl', 'rb') as ifp:
    print(pickle.load(ifp))

# dump information to that file
pickle.dump(RF_Airticket_price_prediction,file)

model=open('RF_Airticket_price_prediction.pkl','rb')
forest=pickle.load(model)

y_prediction4=forest.predict(X)

y_prediction4

metrics.r2_score(y,y_prediction4)

yfinal=pd.DataFrame(data=y_prediction4)

yfinal=yfinal.rename(columns={0: "preds"})

yfinal.describe()

#took random samples for better vizualization
showIntervals(RF_actual_pred)

heights=y.to_numpy()
weights=y_prediction4

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats


x = heights[:500]
y = weights[:500]

slope, intercept = np.polyfit(x, y, 1)  # linear model adjustment

y_model = np.polyval([slope, intercept], x)   # modeling...

x_mean = np.mean(x)
y_mean = np.mean(y)
n = x.size                        # number of samples
m = 2                             # number of parameters
dof = n - m                       # degrees of freedom
t = stats.t.ppf(0.975, dof)       # Students statistic of interval confidence

residual = y - y_model

std_error = (np.sum(residual**2) / dof)**.5   # Standard deviation of the error

# calculating the r2
# Pearson's correlation coefficient
numerator = np.sum((x - x_mean)*(y - y_mean))
denominator = ( np.sum((x - x_mean)**2) * np.sum((y - y_mean)**2) )**.5
correlation_coef = numerator / denominator
r2 = correlation_coef**2

# mean squared error
MSE = 1/n * np.sum( (y - y_model)**2 )

# to plot the adjusted model
x_line = np.linspace(np.min(x), np.max(x), 100)
y_line = np.polyval([slope, intercept], x_line)

# confidence interval
ci = t * std_error * (1/n + (x_line - x_mean)**2 / np.sum((x - x_mean)**2))**.5
# predicting interval
pi = t * std_error * (1 + 1/n + (x_line - x_mean)**2 / np.sum((x - x_mean)**2))**.5  

###### Ploting

plt.rcParams.update({'font.size': 14})
fig = plt.figure(figsize=(25,10))
ax = fig.add_axes([.1, .1, .8, .8])

ax.plot(x, y, 'o', color = 'royalblue')
ax.plot(x_line, y_line, color = 'royalblue')
ax.fill_between(x_line, y_line + pi, y_line - pi, color = 'lightcyan', label = '95% prediction interval')
ax.fill_between(x_line, y_line + ci, y_line - ci, color = 'skyblue', label = '95% confidence interval')

ax.set_xlabel('actual price')
ax.set_ylabel('predicted price')

# rounding and position must be changed for each case and preference
a = str(np.round(intercept))
b = str(np.round(slope,2))
r2s = str(np.round(r2,2))
MSEs = str(np.round(MSE))

#ax.text(45, 110, 'y = ' + a + ' + ' + b + ' x')

print('/n')
ax.text(45, 100, '$r^2$ = ' + r2s + '     MSE = ' + MSEs)

plt.legend(bbox_to_anchor=(1, .25), fontsize=12)

#plotting 500 values for better visualization.

# Pearson's correlation coefficient
numerator = np.sum((x - x_mean)*(y - y_mean))
denominator = ( np.sum((x - x_mean)**2) * np.sum((y - y_mean)**2) )**.5
correlation_coef = numerator / denominator
r2 = correlation_coef**2
r2